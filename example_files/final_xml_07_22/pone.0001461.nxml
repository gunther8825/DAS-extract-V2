<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">18213370</article-id><article-id pub-id-type="pmc">2180191</article-id><article-id pub-id-type="publisher-id">07-PONE-RA-02300R1</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0001461</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computational Biology/Computational Neuroscience</subject><subject>Neuroscience/Behavioral Neuroscience</subject><subject>Neuroscience/Motor Systems</subject><subject>Neuroscience/Theoretical Neuroscience</subject></subj-group></article-categories><title-group><article-title>A Technique for Characterizing the Development of Rhythms in Bird Song</article-title><alt-title alt-title-type="running-head">Characterize Rhythms</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Saar</surname><given-names>Sigal</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref><xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref><xref ref-type="corresp" rid="cor1">
<sup>&#x0002a;</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Mitra</surname><given-names>Partha P.</given-names></name><xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref></contrib></contrib-group><aff id="aff1">
<label>1</label>
<addr-line>Department of Biology, The City College of New York, City University of New York, New York, New York, United States of America</addr-line>
</aff><aff id="aff2">
<label>2</label>
<addr-line>Cold Spring Harbor Laboratory, Cold Spring Harbor, New York, United States of America</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>McCabe</surname><given-names>Brian</given-names></name><role>Academic Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">Columbia University, United States of America</aff><author-notes><corresp id="cor1">&#x0002a; To whom correspondence should be addressed. E-mail: <email>plosone@sigalsaar.com</email></corresp><fn fn-type="con"><p>Conceived and designed the experiments: PM SS. Analyzed the data: SS. Wrote the paper: PM SS.</p></fn></author-notes><pub-date pub-type="collection"><year>2008</year></pub-date><pub-date pub-type="epub"><day>23</day><month>1</month><year>2008</year></pub-date><volume>3</volume><issue>1</issue><elocation-id>e1461</elocation-id><history><date date-type="received"><day>22</day><month>9</month><year>2007</year></date><date date-type="accepted"><day>24</day><month>12</month><year>2007</year></date></history><permissions><copyright-statement>Saar, Mitra.</copyright-statement><copyright-year>2008</copyright-year><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p></license></permissions><abstract><p>The developmental trajectory of nervous system dynamics shows hierarchical structure on time scales spanning ten orders of magnitude from milliseconds to years. Analyzing and characterizing this structure poses significant signal processing challenges. In the context of birdsong development, we have previously proposed that an effective way to do this is to use the dynamic spectrum or spectrogram, a classical signal processing tool, computed at multiple time scales in a nested fashion. Temporal structure on the millisecond timescale is normally captured using a short time Fourier analysis, and structure on the second timescale using song spectrograms. Here we use the dynamic spectrum on time series of song features to study the development of rhythm in juvenile zebra finch. The method is able to detect rhythmic structure in juvenile song in contrast to previous characterizations of such song as unstructured. We show that the method can be used to examine song development, the accuracy with which rhythm is imitated, and the variability of rhythms across different renditions of a song. We hope that this technique will provide a standard, automated method for measuring and characterizing song rhythm.</p></abstract><counts><page-count count="6"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>Developmental learning (for example, speech acquisition in human infants) takes place early in life but its effects may last the entire lifetime of the individual. Developmental learning is difficult to study because the behavioral changes involved span many time scales: Behavioral changes can occur within hours, across daily cycles of wakefulness and sleep and over developmental stages. The study of developmental song learning in birds provides a unique model system for examining this process in detail.</p><p>Previous work has shown that song has structure that spans many time scales <xref rid="pone.0001461-Saar1" ref-type="bibr">[1]</xref>, <xref rid="pone.0001461-Deregnaucourt1" ref-type="bibr">[2]</xref>, <xref rid="pone.0001461-Tchernichovski1" ref-type="bibr">[3]</xref>, <xref rid="pone.0001461-Deregnaucourt2" ref-type="bibr">[4]</xref>. Spectral analysis has proven to be a useful tool in analyzing song temporal structure from milliseconds to several seconds. For example, song spectrograms are the basic tool used to characterize the time-frequency structure of individual songs. Timescales that span several minutes can be analyzed by examining the distribution of syllable features. These distributions reveal stable organized structures (e. g., clusters) even in the early song, where the individual spectrograms appear unstructured. Visual examination of spectrograms and syllable clusters across developmental timescales show the existence of longer time scale structures which have been relatively difficult to quantify.</p><p>We find that at these intermediate timescales, it is useful to quantify the rhythmic patterns present in the vocal production, which we call &#x0201c;song rhythm&#x0201d;. There is no accepted method to measure song rhythms in adult song, let alone juvenile song, which appears unstructured and unstable. We show here, how the song rhythm may be extracted by computing spectrograms of time series composed of song features, and that the &#x0201c;rhythm spectrogram&#x0201d; provides a useful tool to characterize and visualize song development over the entire ontogenetic trajectory.</p><p>There is a pleasing symmetry between the rhythm spectrogram and the song spectrogram, although the latter exhibits the dynamics of the syringeal apparatus and the song system, while the former exhibits developmental dynamics. In the same way that study of the song spectrograms have led to mechanistic insights into song production at the articulatory and neural system level, we expect that the rhythm spectrogram will provide insight into the developmental dynamics of the nervous system, helping to disentangle genetically driven and environmentally driven effects. For example, do juvenile birds have a steady rhythm prior to song learning? Is the rhythm imitated &#x0201c;as is&#x0201d; or does it evolve from an existing rhythm, etc. More generally, investigating rhythm development can help us understand how birds transform their sensory memory of the song they have heard into a set of complex motor gestures that generate an imitation of that song.</p><p>The methods described here are available in the form of MATLAB code distributed as part of the freely available Chronux and Sound Analysis software packages <xref rid="pone.0001461-httpofer.sci.ccny.cuny.eduhtmlsoundanalysis.1" ref-type="bibr">[5]</xref>, <xref rid="pone.0001461-httpwww.chronux.org1" ref-type="bibr">[6]</xref>.</p></sec><sec id="s2"><title>Methods</title><sec id="s2a"><title>Glossary of Terms and Units of Analysis</title><p>The song <bold>bout</bold> is composed of introductory notes followed by a few renditions of a song <bold>motif</bold>. A <bold>syllable</bold> is a continuous sound <xref rid="pone.0001461-Price1" ref-type="bibr">[7]</xref>, <xref rid="pone.0001461-Wild1" ref-type="bibr">[8]</xref>, <xref rid="pone.0001461-Goller1" ref-type="bibr">[9]</xref> bracketed by silent intervals. In this paper we define the <bold>motif duration</bold> as the duration of the syllables and silent intervals, including the silent interval after the last syllable as measured in a song with more than one motif. <xref ref-type="fig" rid="pone-0001461-g001">Figure 1</xref> displays an example of a bout with three motifs where each motif has three syllables.</p><fig id="pone-0001461-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0001461.g001</object-id><label>Figure 1</label><caption><title>A spectrogram of an adult zebra finch song.</title><p>This song has three repetitions of the motif. An occurrence of song is called a bout.</p></caption><graphic xlink:href="pone.0001461.g001"/></fig></sec><sec id="s2b"><title>Multitaper spectral analysis</title><p>We make use of the multitaper framework of spectral analysis <xref rid="pone.0001461-Thomson1" ref-type="bibr">[10]</xref>, <xref rid="pone.0001461-Percival1" ref-type="bibr">[11]</xref>. In addition to robust estimates of spectra and dynamic spectra for signals with complex structure, the multitaper framework also provides robust estimates of time and frequency derivatives of the spectrogram, which we use as the starting point for the computations of song features other than amplitude <xref rid="pone.0001461-Thernichovski1" ref-type="bibr">[12]</xref>.</p></sec><sec id="s2c"><title>Recording and Analysis</title><sec id="s2c1"><title>Subjects &#x00026; training</title><p>We used 48 zebra finches (Taeniopygia guttata) from the City College of New York breeding colony. All birds were kept in social isolation from day 30 to day 90 after hatching. Twelve birds were kept in social isolation and were not exposed to conspecific songs. 36 birds were trained starting from day 43 after hatching with one of three different song playbacks (twelve birds per song model) <xref rid="pone.0001461-Tchernichovski2" ref-type="bibr">[13]</xref>, <xref rid="pone.0001461-httpofer.sci.ccny.cuny.eduhtmlsimilarity.html1" ref-type="bibr">[14]</xref>. The number of playbacks was limited to 10 playbacks per session, two sessions per day. The playbacks were initiated by key pecking. Speakers were placed behind a bird model at the far edge of the cage. Birds were raised from hatching under an artificial photoperiod of 12 h : 12 h LD.</p></sec><sec id="s2c2"><title>Data acquisition</title><p>To facilitate the acquisition and analysis of the continuous recording of song development of individual birds, we have developed an open source software program that automates much of the data acquisition, feature calculation and database handling-Sound Analysis Pro. Song activity was detected automatically and saved (16 bits, sampling frequency 44.1 kHz) continuously throughout the experiment, except when song playbacks were played. We recorded and analyzed 10 terabytes of song, stored as wave files in Lacie external HDs. Songs were analyzed with the batch module of Sound Analysis Pro, and results (for example, millisecond features) were stored in mySQL 4.0 tables (<ext-link ext-link-type="uri" xlink:href="http://mySQL.com">http://mySQL.com</ext-link>). The batch module did spectral analysis and computation of acoustic features using the first and second taper of multitaper spectral analysis <xref rid="pone.0001461-Thomson1" ref-type="bibr">[10]</xref>, <xref rid="pone.0001461-Percival1" ref-type="bibr">[11]</xref> to compute spectral derivatives and acoustic features <xref rid="pone.0001461-httpofer.sci.ccny.cuny.eduhtmlsoundanalysis.1" ref-type="bibr">[5]</xref>. Subsequent analysis was based on the six acoustic features computed on each spectral frame: amplitude, pitch, entropy, FM, continuity and goodness of pitch <xref rid="pone.0001461-Thernichovski1" ref-type="bibr">[12]</xref>. Those features were computed using a 9.27ms advancing in steps of 1ms, effectively smoothing the data with 89.2&#x00025; overlap. Final stages of analysis were performed with MATLAB (The Mathworks, Natick, MA).</p></sec></sec><sec id="s2d"><title>Preliminary Analysis</title><p>The song structure may be summarized using a set of song features such as the amplitude, pitch, mean frequency, amplitude modulation, frequency modulation, continuity in time, continuity in frequency (a definition of these features may be found at <xref rid="pone.0001461-Thernichovski1" ref-type="bibr">[12]</xref>, <xref rid="pone.0001461-Tchernichovski1" ref-type="bibr">[3]</xref>). These features summarize the acoustic structure of the song. In addition, a rhythm analysis summarizing the acoustic structure of specific events in the song can be performed. To do this, a &#x0201c;point process&#x0201d; is calculated, a time series in which all values are zero except at the occurrence of an event, where the value is &#x0201c;one&#x0201d;. An event can be notes, syllables or any kind of temporal marker. For example, in <xref ref-type="fig" rid="pone-0001461-g002">figure 2c</xref>, &#x0201c;one&#x0201d; marks the onset of a syllable. An amplitude threshold was used to identify the onset of syllables. The threshold was chosen and monitored manually with a graphical user interface.</p><fig id="pone-0001461-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0001461.g002</object-id><label>Figure 2</label><caption><title>Regular song spectrograms versus Rhythm spectrograms.</title><p>A. A regular song spectrogram using a 10msec sliding window, showing power up to several kHz. B. Rhythm spectrograms display longer time scales. These are computed by estimating the dynamic spectrum of an appropriate song feature (amplitude in the above example). Each column of the rhythm spectrogram represents the averaged spectrum of song features sung during an hour long interval. C. Rhythm spectrograms that were generated using a point process that marks the onsets of syllables.</p></caption><graphic xlink:href="pone.0001461.g002"/></fig></sec><sec id="s2e"><title>Rhythm analysis</title><p>The spectrogram, i.e. the short-time spectrum computed with a sliding window, has proven in the past to be a good way of looking at the fine temporal structure of songs <xref rid="pone.0001461-Thernichovski1" ref-type="bibr">[12]</xref>. The duration of the sliding window is on the order of 10msec and the spectrum shows power up to several kHz, indicating temporal structure at the millisecond timescale (<xref ref-type="fig" rid="pone-0001461-g002">figure 2A</xref>). Analysis of song features has shown temporal structure in the song over longer timescales, including circadian oscillations <xref rid="pone.0001461-Deregnaucourt1" ref-type="bibr">[2]</xref> and developmental song dynamics <xref rid="pone.0001461-Tchernichovski2" ref-type="bibr">[13]</xref>. One motivation of the current study was to look at these longer timescale dynamics using the same set of tools as was used for the shorter timescale dynamics.</p><p>To look at longer time scales, we use a nested spectral analysis method. First, song feature time series are estimated (see 2.2.3 Preliminary Analysis section). The feature values at a given time point depend on the fine temporal structure of the waveform with millisecond resolution, while the features themselves change with a slower timescale of 10&#x02013;100ms. The continues (not segmented) feature time series are subjected to a second spectral analysis, and the result is a &#x0201c;rhythm&#x0201d; spectrogram, see <xref ref-type="fig" rid="pone-0001461-g002">figure 2B</xref>. In the rhythm spectrogram, the fundamental frequency (that was defined as pitch in a normal spectrogram) is in Hz instead of kHz in the regular spectrogram.</p><p>Rhythm spectrograms can characterize not only continuous and unsegmented song features, but also point process features where each spike (i.e. a &#x0201c;one&#x0201d;) represents the occurrence of a specific event in the song. We use a point process feature when we want to track how a certain temporal marker develops and how stereotypically it occurs. Those temporal markers could be notes, syllables, or onsets/offsets of syllables. For example, <xref ref-type="fig" rid="pone-0001461-g002">figure 2c</xref> shows a feature that marks the onset of syllables.</p><p>We were interested in long time scales on the order of an hour, i.e each column in the rhythm spectrogram would correspond to an hour of singing. A time interval of an hour has many bouts of song followed by silent intervals. The analysis is carried out by first segmenting the time period into song bouts and silence. The segmentation to bouts was done using a very low amplitude threshold that was just above noise level. The threshold levels were chosen manually according to the recording quality. We then perform spectral analysis on the feature time series corresponding to each song bout and then average the song bouts that are sung during an hour. By doing so, we are losing the information on temporal structure between bouts, but the spectral structure within a bout remains.</p><p>From the rhythm spectrogram, we can derive second order features. In the zebra finch, since the main repeating unit is the motif, the fundamental of the rhythm spectrum may be expected to relate to the motif duration. The degree of periodicity of the rhythm may be assessed in the same way as for the regular song spectrum, using the amplitude and width of the corresponding spectral peaks and the Wiener entropy.</p><p>A flowchart of the procedure is shown in <xref ref-type="fig" rid="pone-0001461-g003">figure 3</xref>. The spectrum of the song waveform x(t) is computed to get the song spectrogram S(f,t), or a derivative of that spectrogram <xref rid="pone.0001461-Thernichovski1" ref-type="bibr">[12]</xref>. A feature time series F(t) is derived from the spectrogram to get a coarser time scale representation of the song and subjected to a second round of spectral analysis. The result is a &#x0201c;Rhythm&#x0201d; spectrogram S<sub>R</sub>(f,t) which shows the temporal structure on longer (e.g. developmental) time scales. Second level features may be derived from the &#x0201c;Rhythm&#x0201d; spectrograms (e.g. the song rhythm as defined by the fundamental frequency if the spectrogram shows a harmonic structure).</p><fig id="pone-0001461-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0001461.g003</object-id><label>Figure 3</label><caption><title>A flowchart of the nested spectral analysis as described in the text.</title></caption><graphic xlink:href="pone.0001461.g003"/></fig></sec></sec><sec id="s3"><title>Results</title><p>The adult zebra finch song is composed of a few renditions of the song motif. Each motif has a number of syllables. The rhythm spectrogram shows this repeating structure in the frequency domain, with the fundamental frequency corresponding to the motif duration. In order to verify that this is true, we checked in 20 adult birds, that Indeed, the fundamental of the rhythm spectrograms corresponds to the motif durations. During development there are instances where two types of motifs with two motif durations are sung in one bout, or in different bouts but at the same hour. In those cases, there would be two harmonic trains with different fundamentals. The structure of the harmonics in the rhythm spectrogram, i.e. the energy distribution across the harmonics for one column, is explained by the syllabic structure.</p><p>
<xref ref-type="fig" rid="pone-0001461-g004">Figure 4</xref> shows a rhythm spectrum (<xref ref-type="fig" rid="pone-0001461-g004">figure 4a</xref>) at a developmental stage where the motif duration changes from 270 ms (3.7 Hz) at the age of 47 days, to 400 ms (2.5 Hz)at the age 55, to 600 ms (1.66 Hz) zHYat age 60 (<xref ref-type="fig" rid="pone-0001461-g004">figure 4b</xref>). The transformation from a fundamental of 2.5 Hz to 1.66 Hz, was caused by the incorporation of an additional syllable in the song motif.</p><fig id="pone-0001461-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0001461.g004</object-id><label>Figure 4</label><caption><title>The relations of motif durations and the fundamental frequency of the rhythm spectrogram.</title><p>Changes in the motif duration show up as changes in the fundamental frequency of the rhythm spectrogram as described in the text.</p></caption><graphic xlink:href="pone.0001461.g004"/></fig><p>Sometimes in low frequencies of the rhythm spectrum it is possible to identify song elements (syllables and notes) that correspond to the rhythm. The energy of the corresponding frequency band increases when either the rhythmic component at that frequency range becomes more periodic or its appearance is more frequent. It is possible to distinguish between these two possible causes by looking at the sharpness of the frequency peak. A signal that is less periodic would appear to be smeared, and a signal that is less abundant will look fainter. For example, the most dominant frequency band (around 11Hz) is caused by a short harmonic stack at the beginning of the motif. At day 47, the energy in that frequency band becomes stronger as the short harmonic stack emerges as a distinct syllable. But, as in sonograms, it is not always straightforward to relate the temporal waveform to the frequency patterns observed in the sonogram. Frequency bands in the rhythm spectrogram might not correspond to syllables and notes in a simple and direct way because rhythm is a global feature of the time varying signal.</p><p>The juvenile's song structure can be highly variable, not only in its notes and syllables, but also in its motif composition (<xref ref-type="fig" rid="pone-0001461-g005">Figure 5B</xref>). It is often hard to visually identify a motif, or any repeating unit in the juvenile's song spectrogram. The rhythm spectrogram has proven to be a useful tool in identifying repeated units even in these relatively unstructured songs. <xref ref-type="fig" rid="pone-0001461-g005">Figure 5A</xref> shows the rhythm spectrogram for a juvenile bird, age 47&#x02013;48 days, using the amplitude feature. A strong spectral peak is visible in the rhythm spectrogram at 1.35 Hz. <xref ref-type="fig" rid="pone-0001461-g005">Figure 5B</xref> shows a sample of songs from the same days. It is hard to identify by eye any repeating unit in the song spectrogram, but a periodicity of 740msec (corresponding to 1.35Hz) may be found in the onsets of song syllables (highlighted by the black lines- <xref ref-type="fig" rid="pone-0001461-g005">figure 5B</xref>).</p><fig id="pone-0001461-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0001461.g005</object-id><label>Figure 5</label><caption><title>Rhythm of juvenile songs.</title><p>The rhythm of juvenile song can be identified early during development, as described in the text.</p></caption><graphic xlink:href="pone.0001461.g005"/></fig></sec><sec id="s4"><title>Discussion</title><p>In this paper we have presented a method that nests spectral analysis across timescales to study longer time scale structure in birdsong development. This technique can detect rhythm early in the zebra finch song development, and can track the transition from the juvenile rhythms to the adult rhythms which correspond to the song motif. The study of rhythm development should provide a different perspective from the one where attention is paid to template matching at the level of the spectral frame <xref rid="pone.0001461-Deregnaucourt2" ref-type="bibr">[4]</xref>, <xref rid="pone.0001461-Tchernichovski2" ref-type="bibr">[13]</xref>. It also promises to provide mechanistic insight into the development of the song circuitry, in the same way that the study of song spectrograms has provided mechanistic insight into the dynamics of the peripheral apparatus that produces song <xref rid="pone.0001461-Wild1" ref-type="bibr">[8]</xref>, <xref rid="pone.0001461-Goller1" ref-type="bibr">[9]</xref>.</p></sec></body><back><ack><p>We would like to thank Ofer Tchernichovski, Sebastien Deregnaucourt, Olga Feher, Kristen Maul, Peter Andrews and David Par for their help with this work.</p></ack><fn-group><fn fn-type="COI-statement"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="financial-disclosure"><p><bold>Funding: </bold>This research was supported by Crick-Clay professorship, NIH grant 5R01MH071744-03, US Public Health Services (PHS) grants DC04722-07, NS050436 and by a NIH RCMI grant G12RR-03060 to CCNY. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p></fn></fn-group><ref-list><title>References</title><ref id="pone.0001461-Saar1"><label>1</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Saar</surname><given-names>S</given-names></name><name><surname>Tchernichovski</surname><given-names>O</given-names></name><name><surname>Mitra</surname><given-names>PP</given-names></name><name><surname>Feher</surname><given-names>O</given-names></name></person-group>
<year>2005</year>
<article-title>The development of rhythm and syntax in the Zebra Finch song.</article-title>
<source>SFN 2005</source>
</element-citation></ref><ref id="pone.0001461-Deregnaucourt1"><label>2</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Deregnaucourt</surname><given-names>S</given-names></name><name><surname>Mitra</surname><given-names>PP</given-names></name><name><surname>Feher</surname><given-names>O</given-names></name><name><surname>Pytte</surname><given-names>C</given-names></name><name><surname>Tchernichovski</surname><given-names>O</given-names></name></person-group>
<year>2005</year>
<article-title>How sleep affects the developmental learning of bird song.</article-title>
<source>Nature</source>
<fpage>710</fpage>
<lpage>716</lpage>
<comment>Vol. 433, No. 7027. (17 February 2005)</comment>
</element-citation></ref><ref id="pone.0001461-Tchernichovski1"><label>3</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Tchernichovski</surname><given-names>O</given-names></name><name><surname>Lints</surname><given-names>T</given-names></name><name><surname>Deregnaucourt</surname><given-names>S</given-names></name><name><surname>Mitra</surname><given-names>PP</given-names></name></person-group>
<year>2004</year>
<article-title>Analysis of the entire song development: Methods and Rationale.</article-title>
<source>Annals of the New York Academy of Science.</source>
<volume>1016</volume>
<fpage>348</fpage>
<lpage>363 special issue: Neurobiology of Birdsong, Eds. Ziegler &#x00026; Marler</lpage>
</element-citation></ref><ref id="pone.0001461-Deregnaucourt2"><label>4</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Deregnaucourt</surname><given-names>S</given-names></name><name><surname>Mitra</surname><given-names>PP</given-names></name><name><surname>Lints</surname><given-names>T</given-names></name><name><surname>Tchernichovski</surname><given-names>O</given-names></name></person-group>
<year>2004</year>
<article-title>Song development: In Search for the error-signal.</article-title>
<source>Ann NY Acad Sci 2004</source>
<volume>1016</volume>
<fpage>364</fpage>
<lpage>376 Special issue: Neurobiology of Birdsong, Eds. Ziegler &#x00026; Marler</lpage>
</element-citation></ref><ref id="pone.0001461-httpofer.sci.ccny.cuny.eduhtmlsoundanalysis.1"><label>5</label><mixed-citation publication-type="book">
<comment><ext-link ext-link-type="uri" xlink:href="http://ofer.sci.ccny.cuny.edu/html/sound_analysis.html">http://ofer.sci.ccny.cuny.edu/html/sound_analysis. html</ext-link></comment>
</mixed-citation></ref><ref id="pone.0001461-httpwww.chronux.org1"><label>6</label><mixed-citation publication-type="book">
<comment><ext-link ext-link-type="uri" xlink:href="http://www.chronux.org/">http://www.chronux.org/</ext-link></comment>
</mixed-citation></ref><ref id="pone.0001461-Price1"><label>7</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Price</surname><given-names>P</given-names></name></person-group>
<year>1979</year>
<article-title>Developmental determinants of structure in zebra finch song.</article-title>
<source>J Comp Physiol Psychol</source>
<volume>93</volume>
<fpage>260</fpage>
</element-citation></ref><ref id="pone.0001461-Wild1"><label>8</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Wild</surname><given-names>JM</given-names></name><name><surname>Goller</surname><given-names>F</given-names></name><name><surname>Suthers</surname><given-names>RA</given-names></name></person-group>
<year>1998</year>
<article-title>Inspiratory muscle activity during bird song.</article-title>
<source>J Neurobiol</source>
<volume>36</volume>
<fpage>441</fpage>
<lpage>453</lpage>
<pub-id pub-id-type="pmid">9733078</pub-id></element-citation></ref><ref id="pone.0001461-Goller1"><label>9</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Goller</surname><given-names>F</given-names></name><name><surname>Cooper</surname><given-names>BG</given-names></name></person-group>
<year>2004</year>
<article-title>Peripheral motor dynamics of song production in the zebra finch.</article-title>
<source>Ann NY Acad Sci</source>
<volume>1016</volume>
<fpage>130</fpage>
<lpage>152</lpage>
<pub-id pub-id-type="pmid">15313773</pub-id></element-citation></ref><ref id="pone.0001461-Thomson1"><label>10</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Thomson</surname><given-names>D</given-names></name></person-group>
<year>1982</year>
<article-title>Spectrum estimation and harmonic analysis.</article-title>
<source>Proceedings of the Institute of Electrical and Electronics Engineers 70</source>
<fpage>1055</fpage>
<lpage>1096</lpage>
</element-citation></ref><ref id="pone.0001461-Percival1"><label>11</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Percival</surname><given-names>DB</given-names></name><name><surname>Walden</surname><given-names>AT</given-names></name></person-group>
<year>1993</year>
<article-title>Spectral Analysis for Physical Applications: Multitaper and Conventional Univariate Techniques,</article-title>
<publisher-loc>Cambridge</publisher-loc>
<publisher-name>Cambridge University Press</publisher-name>
</element-citation></ref><ref id="pone.0001461-Thernichovski1"><label>12</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Thernichovski</surname><given-names>O</given-names></name><name><surname>Nottebohm</surname><given-names>F</given-names></name><name><surname>Ho</surname><given-names>CE</given-names></name><name><surname>Bijan</surname><given-names>P</given-names></name><name><surname>Mitra</surname><given-names>PP</given-names></name></person-group>
<year>2000</year>
<article-title>A procedure for an automated measurement of song similarity.</article-title>
<source>Animal Behavior</source>
<volume>59</volume>
<fpage>1167</fpage>
<lpage>1176</lpage>
</element-citation></ref><ref id="pone.0001461-Tchernichovski2"><label>13</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Tchernichovski</surname><given-names>O</given-names></name><name><surname>Mitra</surname><given-names>PP</given-names></name><name><surname>Lints</surname><given-names>T</given-names></name><name><surname>Nottebohm</surname><given-names>F</given-names></name></person-group>
<year>2001</year>
<article-title>Dynamics of the vocal imitation process: how a zebra finch learns its song.</article-title>
<source>Science</source>
<volume>291</volume>
<fpage>2564</fpage>
<lpage>2569</lpage>
<pub-id pub-id-type="pmid">11283361</pub-id></element-citation></ref><ref id="pone.0001461-httpofer.sci.ccny.cuny.eduhtmlsimilarity.html1"><label>14</label><mixed-citation publication-type="book">
<comment><ext-link ext-link-type="uri" xlink:href="http://ofer.sci.ccny.cuny.edu/html/similarity.html">http://ofer.sci.ccny.cuny.edu/html/similarity.html</ext-link></comment>
</mixed-citation></ref></ref-list></back></article>